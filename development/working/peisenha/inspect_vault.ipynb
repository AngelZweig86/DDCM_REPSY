{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I run all regression tests with the develop branch before my updates, I get the following: \n",
    "\n",
    "\"Failure during regression testing @socrates for test(s): 1700, 3184, 5063, 9923.\"\n",
    "\n",
    "When I run the first 200 tests after my updates (including your hotfix) I get only about twenty percent failures.\n",
    "\n",
    "(After the updates the ambiguity section of the test dictionary is just ignored.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the regression_vault:\n",
    "with open('../../respy/tests/resources/regression_vault.respy.json') as j:\n",
    "    vault = json.load(j)\n",
    "    \n",
    "# list of tests that failed after the update:\n",
    "failed = [1, 4, 11, 19, 34, 37, 46, 55, 62, 63, 67, 70, 81, 85, 88, 127,\n",
    "          131, 135, 137, 139, 145, 147, 149, 151, 155, 164, 165, 167, 175,\n",
    "          180, 182, 183, 184, 185, 194, 199.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the non-failures are not surprising, since some models don't use ambiguity, have myopic agents or only one period. According to those criteria I construct a list of tests that should fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 11, 16, 19, 24, 34, 37, 44, 46, 55, 62, 67, 70, 81, 85, 92, 98, 102, 115, 117, 127, 131, 135, 137, 139, 145, 147, 149, 151, 155, 164, 165, 167, 168, 175, 180, 183, 184, 185, 194]\n"
     ]
    }
   ],
   "source": [
    "should_fail = []\n",
    "for i, (test, _) in enumerate(vault[:200]):\n",
    "    expected_to_pass = False\n",
    "    # I'm not completely sure what to expect if the ambiguity coeff is 0 but not fixed\n",
    "    # In practice most of those cases run through.\n",
    "    if test['AMBIGUITY']['coeffs'][0] == 0.0: # and test['AMBIGUITY']['fixed'][0] is True:\n",
    "        expected_to_pass = True\n",
    "    if test['BASICS']['periods'] == 1:\n",
    "        expected_to_pass = True\n",
    "    if test['BASICS']['coeffs'][0] == 0.0:\n",
    "        expected_to_pass = True\n",
    "    \n",
    "    if not expected_to_pass:\n",
    "        should_fail.append(i)\n",
    "        \n",
    "print(should_fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we construct a list of tests that should fail, according to the criteria but don't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 24, 44, 92, 98, 102, 115, 117, 168]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprisingly_passing = []\n",
    "for test in should_fail:\n",
    "    if test not in failed:\n",
    "        surprisingly_passing.append(test)\n",
    "surprisingly_passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "I did not find a strong pattern among surpsisingly passing tests but many of them have either a very low delta or a very low ambiguity level or both. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "source": [
    "Next we construct a list of tests that should not fail but do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63, 88, 182, 199.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprisingly_failing = []\n",
    "for test in failed:\n",
    "    if test not in should_fail:\n",
    "        surprisingly_failing.append(test)\n",
    "surprisingly_failing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
